# Architecture Overview

## System Design

The API Docs Scraper is now a **dual-mode application** supporting both CLI and REST API:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Application Entry                         â”‚
â”‚                        (Application.java)                         â”‚
â”‚  - Spring Boot Application                                        â”‚
â”‚  - Auto-detection: CLI mode vs Web mode                           â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                                      â”‚
     â–¼                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Mode       â”‚                          â”‚   REST API Mode     â”‚
â”‚  (ScraperCLI)    â”‚                          â”‚  (Spring Boot)      â”‚
â”‚                  â”‚                          â”‚                     â”‚
â”‚  - Picocli args  â”‚                          â”‚  - POST /scrape    â”‚
â”‚  - File output   â”‚                          â”‚  - JSON response   â”‚
â”‚  - Single URL    â”‚                          â”‚  - Async support   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Orchestration Layer         â”‚
                â”‚   (ApiDocsScraper)            â”‚
                â”‚                               â”‚
                â”‚  - URL fetching               â”‚
                â”‚  - Parser selection           â”‚
                â”‚  - LLM integration            â”‚
                â”‚  - Error handling             â”‚
                â”‚  - Multi-URL support          â”‚
                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           â”‚           â”‚                â”‚
          â–¼           â–¼           â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Fetcher â”‚  â”‚ Parser â”‚  â”‚ LLM     â”‚   â”‚ Exporter â”‚
    â”‚ Layer  â”‚  â”‚ Layer  â”‚  â”‚ Layer   â”‚   â”‚ Layer    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Layer Details

### 1. Model Layer (`com.freshworks.scraper.model`)

**Purpose**: Define domain objects for representing API documentation structure.

**Components**:
- `Endpoint`: Represents an API endpoint with method, path, parameters, and examples
- `Parameter`: Represents a parameter (query, path, header, body)
- `Example`: Represents code examples (request/response)
- `ScrapedResult`: Container for all scraped endpoints from a source

**Design Decisions**:
- POJOs with getters/setters for easy JSON serialization
- Mutable objects for builder-pattern construction during parsing
- Rich domain model with helper methods (addParameter, addTag)

### 2. Fetcher Layer (`com.freshworks.scraper.fetcher`)

**Purpose**: Retrieve HTML content from documentation URLs.

**Components**:
- `PageFetcher` (interface): Abstract fetching contract
- `HttpClientFetcher`: Fast HTTP-based fetching for static pages
- `PlaywrightFetcher`: Browser-based fetching for JS-rendered pages
- `FetcherFactory`: Smart factory for selecting appropriate fetcher

**Design Decisions**:
- Strategy pattern for swappable fetching implementations
- Factory pattern for intelligent fetcher selection
- Resource management (closeable interface) for Playwright cleanup
- Auto-detection of JS-heavy sites by URL patterns

**Flow**:
```
URL â†’ FetcherFactory â†’ Fetcher Selection
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                               â–¼
    HttpClientFetcher            PlaywrightFetcher
    (static HTML)                (JS-rendered)
         â”‚                               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
                  HTML Content
```

### 3. Parser Layer (`com.freshworks.scraper.parser`)

**Purpose**: Extract structured endpoint information from HTML using multiple parsing strategies including LLM-enhanced extraction.

**Components**:
- `DocumentParser` (interface): Parsing contract
- `JsoupParser`: Jsoup-based implementation with DOM parsing strategies
- `StoplightParser`: Specialized parser for Stoplight-based documentation
- `LLMSmartParser`: Generic LLM-powered parser for any complex documentation
- `LLMEnhancedParser`: Intelligent parser that uses LLM to discover and extract endpoints

**Parser Selection Strategy**:
```
URL Analysis â†’ Parser Selection
    â”‚
    â”œâ”€â†’ LLM available â†’ LLMSmartParser (works with any documentation)
    â”‚
    â”œâ”€â†’ Stoplight detected (no LLM) â†’ StoplightParser
    â”‚
    â””â”€â†’ Default â†’ JsoupParser
```

**LLMSmartParser Flow** (LLM-Enhanced):
```
1. Fetch base documentation page
   â”‚
2. Ask LLM: "Discover all API endpoints on this page"
   â”‚
3. LLM returns list of endpoint metadata
   â”‚
4. For each endpoint:
   â”‚   â”œâ”€â†’ Fetch dedicated documentation page (if available)
   â”‚   â”œâ”€â†’ Ask LLM: "Extract complete details for this endpoint"
   â”‚   â””â”€â†’ Parse LLM response into Endpoint object
   â”‚
5. Return all discovered endpoints
```

**Parsing Strategies** (JsoupParser):
1. **Header-based parsing**: Looks for headings containing HTTP methods
2. **Code block parsing**: Extracts endpoints from code examples
3. **Table parsing**: Finds parameter tables and extracts metadata

**Design Decisions**:
- LLM integration for complex documentation sites
- Multiple complementary strategies to maximize coverage
- Automatic parser selection based on URL and content
- Retry logic for transient LLM or fetch failures
- Graceful fallback to basic parsing if LLM fails

### 4. Exporter Layer (`com.freshworks.scraper.exporter`)

**Purpose**: Export scraped data to structured formats.

**Components**:
- `JsonExporter`: Gson-based JSON serialization

**Design Decisions**:
- Pretty-printed JSON for human readability
- Include null values for complete schema visibility
- File-based and string-based export options
- Extensible design (future: YAML, CSV exporters)

### 5. LLM Integration Layer (`com.freshworks.scraper.llm`)

**Purpose**: Use Large Language Models for intelligent endpoint discovery and extraction.

**Components**:
- `LLMClient`: HTTP client for Freshworks LLM API (Claude 4 Sonnet)
- `LLMException`: Custom exception for LLM-related errors

**Features**:
- **Endpoint Discovery**: LLM identifies all API endpoints from documentation pages
- **Detail Extraction**: LLM extracts complete endpoint details (parameters, examples, descriptions)
- **Configurable API**: Token-based authentication, configurable URLs and models
- **Retry Mechanism**: Automatic retries (3 attempts) for transient failures
- **Comprehensive Logging**: Detailed logs for all LLM API calls with timing
- **Response Handling**: Robust parsing of various LLM response formats

**LLM API Configuration**:
- URL: `https://cloudverse.freshworkscorp.com/api/v2/chat`
- Model: `anthropic-claude-4-sonnet`
- Temperature: 0 (deterministic responses)
- Timeout: 120 seconds
- Token: Configurable via `application.properties` or environment variable

**LLM Flow**:
```
Parse Request
    â”‚
    â”œâ”€â†’ Detect if LLM needed (Calendly, Stoplight, etc.)
    â”‚
    â”œâ”€â†’ Initialize LLMClient with token
    â”‚
    â”œâ”€â†’ For each LLM call:
    â”‚   â”‚   â”œâ”€â†’ Build system + user prompts
    â”‚   â”‚   â”œâ”€â†’ Send HTTP POST request
    â”‚   â”‚   â”œâ”€â†’ Wait for response (with timeout)
    â”‚   â”‚   â”œâ”€â†’ Parse JSON response
    â”‚   â”‚   â”œâ”€â†’ Handle markdown code blocks
    â”‚   â”‚   â””â”€â†’ Return structured data
    â”‚
    â””â”€â†’ Retry on failures (up to 3 attempts)
```

### 6. Web Layer (`com.freshworks.scraper.web`) - Spring Boot

**Purpose**: REST API for programmatic access to scraping functionality.

**Components**:
- `Application`: Spring Boot entry point with dual-mode operation
- `ApiScraperController`: REST controller for scraping endpoints
- `ScrapeRequest`: Request DTO with validation
- `ScrapeResponse`: Response DTO with standardized format

**REST Endpoints**:
- `POST /api/v1/scraper/scrape` - Main scraping endpoint
- `GET /api/v1/scraper/health` - Health check
- `GET /api/v1/scraper/config` - Configuration info

**Request Format**:
```json
{
  "url": "https://developer.example.com/api-docs",
  "usePlaywright": true,
  "verbose": false,
  "llmToken": "optional-token-override"
}
```

**Response Format**:
```json
{
  "success": true,
  "message": "Successfully scraped 5 endpoints",
  "data": {
    "sourceUrl": "...",
    "endpointCount": 5,
    "endpoints": [...]
  }
}
```

### 7. Configuration Layer (`com.freshworks.scraper.config`)

**Purpose**: Manage application configuration and LLM settings.

**Components**:
- `AppConfig`: Spring Boot configuration component with `@Value` injection
- `application.properties`: Configuration file

**Configuration Priority**:
1. Command-line arguments
2. Environment variables
3. `application.properties` file
4. Default values

**Key Settings**:
- `llm.api.token`: LLM API authentication token
- `llm.api.url`: LLM API endpoint URL
- `llm.api.model`: LLM model name
- `server.port`: Spring Boot server port (default: 8080)

### 8. CLI Layer (`com.freshworks.scraper.cli`)

**Purpose**: User-facing command-line interface (legacy, still supported).

**Components**:
- `ScraperCLI`: Picocli-based CLI with argument parsing

**Features**:
- Multiple input methods (--url flag, positional args)
- Sensible defaults (output file, fetcher selection)
- Help and version commands
- Verbose mode for debugging
- Proper exit codes for scripting
- LLM token configuration support

## Data Flow

### Complete Scraping Flow (REST API Mode)

```
1. REST Request
   POST /api/v1/scraper/scrape
   {
     "url": "https://developer.example.com/api-docs/...",
     "usePlaywright": true
   }

2. Controller Layer
   â””â”€â†’ ApiScraperController.scrape()
       â”œâ”€â†’ Load LLM token from config
       â”œâ”€â†’ Initialize ApiDocsScraper with LLM
       â””â”€â†’ Call scraper.scrape(url)

3. Orchestration (ApiDocsScraper)
   â”œâ”€â†’ Fetch page: PlaywrightFetcher
   â”‚   â”œâ”€â†’ Check browser availability
   â”‚   â”œâ”€â†’ Create new page
   â”‚   â”œâ”€â†’ Navigate with timeout (60s)
   â”‚   â”œâ”€â†’ Wait for load state
   â”‚   â”œâ”€â†’ Wait for network idle
   â”‚   â”œâ”€â†’ Scroll to trigger lazy loading
   â”‚   â””â”€â†’ Return HTML content
   â”‚
   â””â”€â†’ Parser Selection
       â”œâ”€â†’ Analyze URL and HTML
       â””â”€â†’ Select LLMSmartParser (if LLM available)

4. LLM-Enhanced Parsing (LLMSmartParser)
   Step 1: Discovery
   â”œâ”€â†’ Extract text from HTML (max 10k chars)
   â”œâ”€â†’ Call LLM: "Discover all API endpoints"
   â”œâ”€â†’ LLM returns JSON array of endpoints
   â””â”€â†’ Parse endpoint list
   
   Step 2: Detailed Extraction
   For each endpoint:
   â”œâ”€â†’ Fetch dedicated page (if URL available)
   â”œâ”€â†’ Call LLM: "Extract complete endpoint details"
   â”œâ”€â†’ Parse LLM response
   â”œâ”€â†’ Extract parameters, examples, descriptions
   â””â”€â†’ Create Endpoint object
   
   â””â”€â†’ Return List<Endpoint>

5. Result Building
   â””â”€â†’ ScrapedResult
       â”œâ”€â†’ Add endpoints
       â”œâ”€â†’ Add metadata (source, timestamp, count)
       â””â”€â†’ Calculate stats

6. Response
   â””â”€â†’ ApiScraperController
       â”œâ”€â†’ Wrap in ScrapeResponse
       â”œâ”€â†’ Add success status
       â””â”€â†’ Return JSON to client
```

### CLI Mode Flow

```
1. CLI Execution
   â””â”€â†’ Application.main() detects CLI mode
       â””â”€â†’ Call ScraperCLI.main(args)

2. CLI Parsing
   â””â”€â†’ Picocli parses arguments
       â”œâ”€â†’ --url, --output, --render, --llm-token
       â””â”€â†’ Build scraping request

3. Scraping (Same as REST mode)
   â””â”€â†’ ApiDocsScraper.scrape()
       â””â”€â†’ Fetch â†’ Parse â†’ Build Result

4. Export
   â””â”€â†’ JsonExporter.export()
       â”œâ”€â†’ Serialize to JSON
       â”œâ”€â†’ Write to file
       â””â”€â†’ Print success message

5. Exit
   â””â”€â†’ Proper exit code (0 = success, 1 = error)
```

### LLM Integration Flow

```
LLMSmartParser (Generic, works with any documentation)
    â”‚
    â”œâ”€â†’ Initial Discovery Request
    â”‚   â”œâ”€â†’ Build system prompt: "You are an API documentation expert..."
    â”‚   â”œâ”€â†’ Build user prompt: "Analyze this API documentation page and list ALL endpoints..."
    â”‚   â”œâ”€â†’ LLMClient.call()
    â”‚   â”œâ”€â†’ HTTP POST to Freshworks API
    â”‚   â”œâ”€â†’ Wait for response (with 120s timeout)
    â”‚   â”œâ”€â†’ Parse JSON response
    â”‚   â””â”€â†’ Extract endpoint list
    â”‚
    â””â”€â†’ Detailed Extraction (for each endpoint)
        â”œâ”€â†’ Build system prompt: "Extract complete endpoint details..."
        â”œâ”€â†’ Build user prompt: "Extract details for GET /users..."
        â”œâ”€â†’ LLMClient.call()
        â”œâ”€â†’ Retry on failure (up to 3 attempts)
        â”œâ”€â†’ Parse structured response
        â””â”€â†’ Build Endpoint object with parameters
```

## Error Handling Strategy

### Retry Mechanisms

**Playwright Fetcher**:
- âœ… 3 automatic retries on browser close errors
- âœ… 2-second delay between retries
- âœ… 60-second navigation timeout
- âœ… Browser availability checks

**LLM Client**:
- âœ… 3 automatic retries on HTTP errors
- âœ… 120-second request timeout
- âœ… Graceful fallback if LLM unavailable
- âœ… Detailed logging for debugging

### Graceful Degradation

The scraper continues processing even when individual endpoints fail:

```
Scrape Operation
    â”‚
    â”œâ”€â†’ Endpoint 1: âœ… Success
    â”‚
    â”œâ”€â†’ Endpoint 2: âŒ LLM error â†’ Retry â†’ âœ… Success
    â”‚
    â”œâ”€â†’ Endpoint 3: âŒ Fetch error â†’ Retry â†’ âœ… Success
    â”‚
    â””â”€â†’ Endpoint 4: âŒ All retries failed â†’ Skip and continue

Result: 3 of 4 endpoints successfully extracted
```

### Exception Hierarchy

```
Exception
  â””â”€â†’ ScraperException
       â”œâ”€â†’ FetchException (thrown by fetchers)
       â”‚    â”œâ”€â†’ PlaywrightFetcher: retry on browser errors
       â”‚    â””â”€â†’ HttpClientFetcher: network errors
       â””â”€â†’ LLMException (thrown by LLM calls)
            â””â”€â†’ Network errors, timeout, invalid response
```

### Logging Strategy

**Comprehensive Logging Throughout**:
- ğŸš€ Operation start/end with timing
- ğŸ“ URL being processed
- ğŸ” Parser selection
- ğŸ¤– LLM API calls with timing
- ğŸ“Š Progress (e.g., "Processing endpoint [1/15]")
- âœ… Success confirmations
- âš ï¸ Warnings for retries
- âŒ Errors with context

**Log Levels**:
- `INFO`: Main operations, progress, results
- `DEBUG`: Detailed step-by-step information
- `WARN`: Recoverable issues (fallbacks)
- `ERROR`: Failures that affect the operation

## Testing Strategy

### Test Coverage

1. **Unit Tests**
   - `JsoupParserTest`: Tests all parsing strategies with sample HTML
   - `JsonExporterTest`: Tests JSON serialization and file export
   - `LLMPromptBuilderTest`: Tests prompt generation

2. **Test Resources**
   - `sample-api-doc.html`: Realistic API documentation sample
   - Contains multiple endpoints with parameters and examples

3. **Integration Testing** (Future)
   - End-to-end scraping of real documentation sites
   - Comparison with golden files

## Extension Points

### Adding New Fetchers

Implement the `PageFetcher` interface:

```java
public class CustomFetcher implements PageFetcher {
    @Override
    public String fetch(String url) throws FetchException {
        // Custom fetching logic
    }
    
    @Override
    public void close() {
        // Cleanup
    }
}
```

### Adding New Parsers

Implement the `DocumentParser` interface:

```java
public class CustomParser implements DocumentParser {
    @Override
    public List<Endpoint> parse(String html, String sourceUrl) {
        // Custom parsing logic
    }
}
```

### Adding New Exporters

Follow the pattern of `JsonExporter`:

```java
public class YamlExporter {
    public void export(ScrapedResult result, Path outputPath) {
        // YAML export logic
    }
}
```

## Performance Considerations

### Playwright Overhead

Playwright fetching is significantly slower than HTTP fetching:
- HTTP: ~100-500ms per page
- Playwright: ~3-10 seconds per page (includes browser startup, JS execution)

**Mitigation**:
- Only use Playwright when necessary (--render flag or auto-detection)
- Reuse browser instance across multiple pages (implemented)
- Configurable wait times

### Memory Usage

For large documentation sites:
- Each page's HTML is loaded into memory
- Parsed endpoint objects are retained until export
- Playwright browser process adds ~100-200MB

**Mitigation**:
- Process and export URLs in batches (future enhancement)
- Increase JVM heap size for large jobs: `java -Xmx2g -jar ...`

## Security Considerations

1. **SSL/TLS**: Trusts system certificate store
2. **Redirects**: Follows redirects automatically (HTTP client)
3. **User-Agent**: Identifies as "API-Docs-Scraper/1.0"
4. **Rate Limiting**: Not implemented (future enhancement for respectful scraping)

## Current Implementation Status

### âœ… Completed Features

1. **Spring Boot REST API**: Full REST API with health checks
2. **LLM Integration**: Claude 4 Sonnet via Freshworks API
3. **Enhanced Logging**: Comprehensive logs with timing and progress
4. **Retry Mechanisms**: Automatic retries for transient failures
5. **Parser Selection**: Intelligent parser selection based on URL and content
6. **Calendly Support**: Specialized parser for Calendly's complex documentation
7. **Playwright Support**: JavaScript rendering with retry logic
8. **Dual Mode**: CLI and REST API in one application

### ğŸ”„ In Progress

1. **Async Processing**: Background job processing for long-running scrapes
2. **Result Caching**: Cache results to avoid re-scraping unchanged docs

### ğŸ“‹ Future Enhancements

1. **Rate Limiting**: Configurable delays between requests
2. **Authentication**: Support for API key or OAuth-protected docs
3. **Caching**: Cache fetched pages to avoid re-downloading
4. **Incremental Updates**: Only scrape changed endpoints
5. **Output Formats**: YAML, CSV, Markdown exporters
6. **OpenAPI Direct Generation**: Generate OpenAPI spec directly
7. **Webhook Support**: Notify on completion of long-running jobs
8. **Multi-language Support**: Extract documentation in multiple languages

